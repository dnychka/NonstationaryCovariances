\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{lineno}
\usepackage{units}
\linespread{1.1}

\journal{Environmetrics}
\bibliographystyle{elsarticle-harv}

%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}

\title{Modeling nonstationary spatial data using local likelihood estimation and Mat\'ern-SAR covariance translation}
\tnotetext[t1]{This document is a collaborative effort. Declarations of interest for all authors: none}
 
\author[CU]{Ashton Wiens\corref{fn1}}
\ead{ashton.wiens@colorado.edu}
\cortext[fn1]{Corresponding author:
Department of Applied Mathematics, 
University of Colorado, 1111 Engineering Dr, Boulder, CO 80309}
\author[CSM]{Douglas Nychka}
\ead{nychka@mines.edu}
\author[CU]{William Kleiber}
\ead{william.kleiber@colorado.edu}
\address[CU]{Department of Applied Mathematics, University of Colorado, Boulder, Colorado, USA}
\address[CSM]{Department of Applied Mathematics and Statistics, Colorado School of Mines, Golden, Colorado, USA}

\address{Colorado, United States}

\begin{abstract}

Modeling data with a nonstationary covariance structure is important to represent heterogeneity in geophysical and other environmental spatial fields. In this work, we investigate a multistage approach to modeling nonstationary covariances that is efficient for large data sets. First, we use likelihood estimation in local, moving windows to infer spatially varying covariance parameters. These surfaces of covariance parameters can then be encoded into a global covariance model specifying the second-order structure for the complete spatial domain, allowing for simulation and prediction. We investigate the nonstationary spatial autoregressive (SAR) model related to Gaussian Markov random field (GMRF) methods, which is amenable to plug in local estimates and practical for large data sets. In addition we use a simulation study to establish the accuracy of local Mat\'ern parameter estimation as a reliable technique when replicate fields are available and small local windows are exploited to reduce computation. This multistage modeling approach is implemented on a nonstationary climate model output data set with the goal of emulating the variation in the model ensemble using a Gaussian process.

\end{abstract}

\begin{keyword}
nonstationary Gaussian process \sep local likelihood \sep Gaussian Markov random field \sep spatial autoregression \sep process convolution
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}

This work is motivated by a climatological application where the goal is to emulate the variability of an ensemble of spatial fields generated by a climate forecast model. Accordingly, we are lead to model spatial data consisting of independent replicated spatial fields that exhibit a nonstationary covariance structure using a Gaussian process. To accurately emulate these fields, modeling the nonstationarity in the second-order structure of the data is essential. 


We investigate a multistage approach to modeling nonstationary covariances, similar to the methodology in \cite{nychka2018modeling}. First, assuming the field is approximately locally stationary, we perform moving window local likelihood estimation to infer spatially varying Mat\'ern covariance parameters. These parameter fields are translated into SAR parameters which best reproduce the behavior of the Mat\'ern correlations locally. Finally, the spatially varying parameters are encoded into the nonstationary SAR covariance model, specifying all of the data jointly.



Local estimation is not a new idea in spatial statistics \cite{haas1990kriging, haas1990lognormal, ver2004flexible, risser2015local}. A local estimation approach circumvents the $\mathcal{O}(n^3)$ computational burden, where $n$ is the number of observations. Instead, the task becomes $n$ parallelizable subproblems on the order of the window size used in the local estimation.

We assume that local estimation is a data-driven approach: when the data consists of densely observed independent replicates, robust local estimation of covariance parameters is possible. In practice, there is often no clear indication of which parameters in the model should be allowed to vary spatially \cite{fuglstad2015does} and what spatial scales are appropriate for the parameter surfaces. This difficult modeling choice is avoided when using local estimation: we can allow all parameters to vary initially, and the local estimates will indicate whether the parameters are constant or vary over space. Furthermore, with local estimation, we do not have to decompose the parameter functions into some prespecified low-dimensional representation \cite{fuglstad2015exploring, risser2016nonstationary}, which can influence the estimation. 

Weighted local likelihoods have been studied to accommodate irregularly spaced observations \cite{anderes2011local}, but in this work we use a simple moving window applied to data on a lattice. Here, we focus on estimation of Mat\'ern parameters, primarily because of their interpretability and in order to study the relationship between the Mat\'ern and SAR covariance models, detailed below. To establish local estimation as a reliable technique, we use a Monte Carlo experiment to study the robustness of local estimation of the correlation range parameter. 


With locally estimated covariance parameters in hand, some care is required to combine these into a valid global nonstationary covariance model. A simple option is to use the estimates to construct local covariance functions and perform local simulation. However, a global covariance specifying the relationships among all of the data is desirable for efficient simulation and necessary for prediction. A global representation also avoids potential artifacts and ad hoc choices in synthesizing the spatial analysis across local windows.



There are several general classes of nonstationary models, such as deformation methods \cite{sampsonguttorp, anderes2008estimating}, basis function methods \cite{cressie2008fixed, katzfuss2011spatio, nychka2015multiresolution, nychka2002multiresolution}, process-convolution construction \cite{higdon1998process, higdon1999non, higdon2002space, paciorek2004nonstationary, fuentes2001new, fuentes2002spectral, zhu2010estimation}, and the SPDE approach \cite{lindgren2011explicit, lindgren2007explicit, simpson2012think, rue2005gaussian}. See \cite{risser2016nonstationary} for a review of nonstationary models and \cite{heaton2017methods} for a review of methods for large spatial data sets. Unfortunately, only a few of these methods are easily implemented due to the complexity of the models. Here, we investigate two existing nonstationary models from the process convolution and GMRF families of methods which are amenable to plug-in local estimates.


%Process convolution is an attractive technique for constructing processes because the positive-definite condition on the covariance matrix is replaced by integrability conditions on the kernel function \cite{higdon1999non}. \citet{paciorek2006spatial} showed that nonstationary Gaussian processes can be constructed by combining a valid isotropic correlation function with a spatially varying anisotropy kernel. This model arguably provides the most straightforward path to encoding the estimated locally stationary Mat\'ern parameters, although being restricted to a Gaussian covariance places strong assumptions on the smoothness of the spatial process. A Gaussian kernel does give a closed form for the covariance but does not lead to any efficiency for large data sets.

In this work, we study the nonstationary spatial autoregressive (SAR) model, related to the Gaussian Markov random field (GMRF) approach to approximating GPs. The idea is to identify members of the Mat\'ern family of spatial processes as solutions to a stochastic partial differential equation. The SPDE is then discretized to a lattice and this motivates the form of the SAR \cite{lindgren2011explicit}. The correspondence between the Mat\'ern/SPDE form and a SAR was presented in \cite{lindgren2011explicit} and an analytical formula was proposed to connect the parameters between the continuous and discrete cases. We have found that the analytical formula is inaccurate for large correlation ranges and one contribution of this work is to sharpen this relationship using numerical results. The advantage is that if one can successfully translate the Mat\'ern formulation into a SAR framework, one can exploit sparse matrix algorithms for fast computation.


Finally, we apply this multistage modeling framework to analyze a nonstationary climate model output data set consisting of 30 temperature anomaly fields from the NCAR CESM project. First, we locally estimate stationary, anisotropic Mat\'ern parameters. We then translate these local Mat\'ern parameters into the SAR parameters which yield the best numerical approximation between the stationary Mat\'ern and the approximately stationary SAR model. Finally, we encode the spatially varying SAR parameters into the nonstationary SAR model. This model convincingly captures many of the nonstationary features of the climate distribution in simulations. 


The paper is organized as follows. In section 2, the Mat\'ern family of correlation functions and the process convolution model are introduced, along with the approximately stationary and nonstationary SAR models from the SPDE approach. We also include a numerical experiment investigating the link between the stationary Mat\'ern and SAR covariance models. In section 3, we develop the local likelihood framework we employ, and we conduct a local estimation simulation study.  In section 4, we apply the multistage modeling framework to analyze a nonstationary climatological data set. We conclude with a summary of the method and a discussion of some of the relevant practical aspects.














\section{Nonstationary covariance models}

 In this section, we introduce the Mat\'ern family of covariance models, as well as the process convolution and the SAR/SPDE approaches to constructing a Gaussian process/GMRF. We then state the connection between the Mat\'ern and SPDE models, and explore this relationship in a numerical study.


\subsection{The Mat\'ern covariance model}

The Mat\'ern family of stationary covariance models is important because of its flexibility and the interpretability of its parameters. The isotropic Mat\'ern covariance function is

$$ C( \, d \, | \, \nu, a, \sigma^2 \, ) =\sigma ^{2}{\frac {2^{1-\nu }}{\Gamma (\nu )}}{(ad)}^{\nu }\mathcal{K}_{\nu }{(ad)}$$

\iffalse
$$ C( \, d \, | \, \nu, a, \sigma^2 \, ) =  {\frac {\pi^{1/2} \sigma ^{2}}{2^{\nu -1 }\Gamma (\nu+1/2 ) a^{2\nu}}} {(ad)}^{\nu }\mathcal{K}_{\nu }{(ad)}$$
\fi

where $\mathcal{K}_{\nu }(\cdot)$ is the modified Bessel function of the second kind of order $\nu$ and $\Gamma(\cdot)$ is the gamma function. $\sigma^2$ is the spatial process variance (sill), $a$ is the multiplicative range parameter, and $\nu$ is the smoothness parameter which controls the mean square differentiability of the process. 

This model can be extended to include geometric anisotropy parameterized by a linear transformation:

\begin{equation}
\label{e:1}
\begin{split}
   A & =  D^{1/2} U \\ 
   \Sigma & = A^T A
  \end{split}
\end{equation}
where $U$ is a rotation matrix parameterized by angle $\theta$

\[ U = \begin{bmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{bmatrix} \]
and

$$D = \begin{bmatrix} \lambda_x & 0 \\ 0 & \lambda_y \end{bmatrix} $$ 
is a diagonal matrix encoding the anisotropic range scaling in the $x$ and $y$ coordinate axes before the rotation has been applied. Then the pairwise Mahalanobis distances among the observation locations can be calculated as follows.

$$ d(\mathbf s, \mathbf s^\prime ) = \sqrt{(\mathbf s - \mathbf s^\prime)^T \Sigma^{-1} (\mathbf s - \mathbf s^\prime)}$$

These distances are then used as input to the isotropic Mat\'ern model with unit range parameter. 


\subsection{Process convolution construction}

Process convolution is a useful method for constructing valid nonstationary covariance models from spatially varying kernels. The kernel must be integrable and square-integrable to ensure a valid covariance, which is a much easier condition to check than the positive-definiteness of the covariance function itself. A stationary or nonstationary Gaussian process defined on $G \subset \mathbb{R}^d$ can be constructed as follows:

$$ Y(\mathbf s) = \int_{G} \Psi_{\mathbf{s}}(\mathbf s - \mathbf u) \, \mathrm d W(\mathbf u ) $$ where $W$ is d-variate Brownian motion or another Gaussian process and $\Psi_{\mathbf{s}}$ is a possibly spatially varying kernel function. 

This integral cannot usually be explicitly computed. However, as shown by \citet{higdon1998process}, the integral has a closed form if a squared exponential correlation function is used with spatially varying kernel matrices $\Sigma_i = \Sigma(\mathbf s_i) \in \mathcal{M}^{d \times d}$ associated with location $\mathbf s_i$ (as in equation (\ref{e:1}) for $d=2$). These matrices control the local range and geometric anisotropy of the process.  

The restriction of this model to the square exponential correlation function is a modeling limitation. \citet{paciorek2004nonstationary} generalized this model, showing that any valid isotropic positive-definite correlation function valid in all dimensions can be used. However, this construction deviates from the process convolution approach.


\subsection{The SAR model}

In contrast to modeling a continuous covariance function, the SAR model parameterizes the precision matrix for the process on a discrete lattice. In our case, we have observations of a spatial field $\mathbf{y}$ located on a regular rectangular grid. This is the setup used throughout the paper and in the data analysis section. 

With this geometry, the conditional dependence relationships (conditional weights) for a single location under one parameterization of the approximately stationary, isotropic SAR model can be written using graphical notation

\begin{equation}
\label{e:2}
    \begin{array}{c|c|c}
      0 & -1 & 0 \\
      \hline
      -1 & 4+\kappa^2 & -1 \\
      \hline
      0 & -1 & 0 \\
    \end{array}
\end{equation} 

which are the conditional weights representing the dependencies among observation locations $\mathbb{E} (y_{ij} | \mathbf{y}_{-ij}) = \frac{1}{4+\kappa^2}(y_{i-1,j} + y_{i+1,j} + y_{i,j-1} + y_{i,j+1})$ and $\operatorname{Var}(y_{ij} | \mathbf{y}_{-ij}) =  \frac{1}{4+\kappa^2}$, where $ \mathbf{y}_{-ij}$ denotes the vector of all observations in $\mathbf y$ except $y_{ij}$. These first and second order conditional relationships are all that is needed to fully specify the GMRF. This formulation implies that the response at the single location under investigation (e.g. $y_{ij}$) is conditionally independent of all other observations given its four first-order neighbors. 

To first step in construct the precision matrix of a SAR process is to flatten/vectorize the spatial stencil into each row of a matrix $B$, one row for each observation in the data. $B$ is convolved with itself, yielding $Q_1 = \frac{1}{\sigma^2} B^T B$, which is the precision matrix of a SAR process with smoothness 1, where $\sigma^2$ is the variance parameter of the model (see the next section on model approximation). If, again, $Q_1$ is convolved with itself, resulting in $Q_2 = Q_1^T Q_1$, then $Q_2$ is the precision matrix for a SAR process with smoothness 2. 

The precision matrix implied by the SAR model is sparse and has a banded structure. This attractive property makes the SAR model amenable to modeling large data sets because the precision matrix can be used instead of a dense covariance matrix for likelihood estimation and simulation. 

Two things should be mentioned about the SAR model. First, this stencil should be modified at the boundaries of the domain. The center value of the stencil should be the $\kappa^2$ plus the sum of the weights of its non-zero neighbors. Second, the value of $\kappa$ affects the marginal variance of the process, so $\sigma^2$ is a parameter that allows for modulation of the variance, but it is not the marginal variance of the process itself.

The approximately stationary, geometrically anisotropic version of the SAR model will be detailed in the next section, after introducing the connection between the SAR and Mat\'ern covariance models.



\subsection{Mat\'ern-SAR model approximation}

Lindgren, Rue, and Lindstr{\"o}m \cite{lindgren2011explicit} suggested that the SAR covariance model can be thought of as a discrete approximation to the Mat\'ern covariance model. The idea is that a Gaussian Markov random field with SAR covariance can provide an approximation to a Gaussian field with Mat\'ern covariance. The connection is established through an SPDE formulation. In particular, it is known that a Gaussian field $u(\mathbf s)$ with stationary Mat\'ern covariance is a solution to the SPDE

$$ (\kappa^2 - \Delta)^{\nicefrac{\alpha}{2}} u(\mathbf s) = \mathcal{W}(\mathbf s) $$
where $\alpha = \nu + \frac{d}{2}$, $\kappa > 0, \nu > 0$, $\mathbf s \in \Omega = \mathbb{R}^d$, $d = 1$ or $2$, and $\mathcal{W}(s) \sim \operatorname{WN}(0, \sigma^2$). As in the Mat\'ern model, $\nu$ controls the smoothness of the Gaussian field. Fixing $\nu = 1$ and $d = 2$, the authors showed that the SAR covariance structure obtained by discretizing the pseudodifferential operator $(\kappa^2 - \Delta)$ approximates a Mat\'ern covariance structure with range $a \approx \kappa$. 

Similar results can be obtained for different smoothness parameters $\nu$ by convolving the finite difference stencil in (\ref{e:2}) with itself $\nu$ times, as detailed in the previous section for $\nu=1$ and $\nu=2$.

This formulation also shows how to extend the SAR model to incorporate geometric anisotropy by including an anisotropy matrix, $H$, in the Laplacian in the pseudodifferential operator.

$$ (\kappa^2 - \nabla \cdot H \nabla)^{\nicefrac{\alpha}{2}} u(\mathbf s) = \mathcal{W}(\mathbf s) $$  

Let 
\[ H = \begin{bmatrix} H_{11} & H_{12} \\
                        H_{12} & H_{22}
        \end{bmatrix} \]
then the first-order finite difference discretization of the anisotropic SPDE gives the following stencil for filling the rows of the $B$ matrix.

\begin{equation}
\label{e:3}
       \arraycolsep=6.0pt\def\arraystretch{2.5}
   \begin{array}{c|c|c}
      \frac{2H_{12}}{h_x h_y} & -\frac{H_{22}}{h_y^2} & -\frac{2H_{12}}{h_x h_y} \\
        \hline
      -\frac{H_{11}}{h_x^2} & \;\; \kappa^2 + \frac{2H_{11}}{h_x^2} + \frac{2H_{22}}{h_y^2} \;\; & -\frac{H_{11}}{h_x^2} \\
      \hline
      -\frac{2H_{12}}{h_x h_y} & -\frac{H_{22}}{h_y^2} & \frac{2H_{12}}{h_x h_y} \\
    \end{array} 
\end{equation}
where $h_x$ and $h_y$ are the grid spacings along the x-axis and y-axis. This is just a reparameterization of the results in Appendix A of \cite{lindgren2011explicit} which facilitates the practical translation of these models.

$H$ extends the SAR model to the anisotropic case, acting similarly to $\Sigma$ in the anisotropic Mat\'ern model. Just as we must fix the range $a$ in the Mat\'ern model in the anisotropic case, we must fix either $\kappa$ or one of the diagonal entries of $H$ to 1 for identifiability in the SAR model.










\subsection{Numerical translation of range parameters between the Mat\'ern and SAR models}
\label{ss:1}

In this section, we attempt to numerically validate analytic results conjecturing the approximation of the Gaussian process Mat\'ern covariance model by the GMRF SAR covariance model. The goal is to ascertain the parameter regime of the Mat\'ern range in which translation between these models is valid in practice. 

The experiment is set up as follows. Given a Mat\'ern range parameter $a$, we attempt to estimate the optimal $\kappa$ in the SAR model which gives the best approximation to the Mat\'ern model. We accomplish this by comparing the correlation matrices for both of these models defined on the same $N \times N$ square grid of spatial locations. We conducted this experiment with the smoothness of the Mat\'ern model fixed at $\nu=1$ and $\nu=2$, and with unit marginal variance for all models. The first step is to fix the Mat\'ern range parameter and encode a Mat\'ern correlation matrix on the grid. Then, we perform an optimization over $\kappa$ by encoding it into the SAR precision matrix using (\ref{e:2}), inverting and normalizing it to give the implied SAR correlation matrix, and minimizing the distance between these matrices by some measure.

It is known that the SAR covariance model suffers from edge effects. To avoid the interference of edge effects in the estimation process, we quantify the difference between the two correlation matrices by only comparing the correlation of the single observation located in the center of the grid for both models. This is encoded in the row corresponding to this observation location in each matrix. We take the $\ell_2$ distance between the rows from each of the two matrices as the distance measure between the model covariance matrices.

We perform this numerical experiment comparing the isotropic Mat\'ern and SAR models for a set of Mat\'ern inverse range parameters $\frac{1}{a} \in [1, 40]$, resulting in a set of optimal $\kappa$ parameters, which are inverted for easier interpretation as well. The results are shown in \ref{f:1}(a) with orange corresponding to the $\nu=1$ case and cyan corresponding to $\nu=2$. The solid black line shows the theoretically conjectured approximation $\frac{1}{a} = \frac{1}{\kappa}$. 



It seems that when the Mat\'ern correlation is long range, the SAR correlation model reproduces this behavior optimally with a larger $\kappa$ value (smaller $\frac{1}{\kappa}$) than is expected from the analytic approximation. We repeated this experiment for many different values of $N$ to assess how the size of the grid and resulting edge effects affect the optimization. This behavior was consistent for all grid sizes we tested up to a $101\times 101$ grid: the approximation becomes better as the grid grows larger but does not converge to the black line at any practical grid size. The results shown in all panels are for $N=73$.

Compared to the Mat\'ern correlation, the relative error of using the SAR correlation with $\kappa$ value derived from the numerical experiment is shown in \ref{f:1}(b). The $\ell_2$ distance measure used in the optimization of the model correlation matrices is used to quantify the resulting model error, normalized by the $\ell_2$ norm of the row from the Mat\'ern correlation matrix.




\begin{figure}
    \centering
    \makebox[0pt]{
        \includegraphics[scale=0.31]{"plots/MaternSARtranslation5".png}
    }
    \caption{For the isotropic case, the optimal $1/ \kappa$ parameter for a given Mat\'ern inverse range $1/a$ is plotted in (a). The relative error incurred by using the SAR model with optimal $\kappa$ as an approximation to the Mat\'ern model is shown in (b). For the anisotropic case, the optimal diagonal values of $H$ are plotted against the fixed diagonal values of $\Sigma$ in panels (c) and (d), and the relative error is shown in (e)}
    \label{f:1}
\end{figure}


To make the Mat\'ern ranges comparable between the model with $\nu=1$ and $\nu=2$, we used the decorrelation range as a proxy. Specifically, for each Mat\'ern range and fixing $\nu=1$, we found the distance at which correlation dropped to 0.05. Then, we found the range of Mat\'ern with smoothness $\nu=2$ which also decorrelated to 0.05 at the same distance. Note that for comparison we have plotted the smoothness $\nu=2$ against the $\nu=1$ range parameters since they are equivalent in the sense just described.

In the data analysis below, we found it necessary to include geometric anisotropy in the covariance model. For this reason, we also investigated how the presence of geometric anisotropy affects the numerical correspondence established for the isotropic case above. The behavior of the approximation was similar to the isotropic case, requiring smaller diagonal values in the anisotropy matrix $H$ than the diagonal values of $\Sigma$ (the off-diagonal elements of both were set to 0 without loss of generality). 

The anisotropic parameter translation results are shown in panels (c) and (d) of Fig \ref{f:1}, with the relative error of approximation shown in panel (e). We tested the length scale ratio $\lambda_x \colon \hspace{-0.4mm}  \lambda_y = 4 \colon \hspace{-1mm} 1$, which was consistent with estimates in the data analysis. In particular, we let $\lambda_x = 1, \cdots, 30$ and $\lambda_y = 4\lambda_x$. The experiment was repeated for 10 rotation angles between $0^{\circ}$ and $90^{\circ}$ with $10^{\circ}$ spacing. The approximation seems may be slightly affected by the rotation angle and oblateness of the geometric anisotropy, but the effect is negligible in practice. From these results, we have ascertained a numerical translation among the anisotropy parameters. We can use these results to translate locally estimated Mat\'ern range parameters into SAR parameters with better accuracy than the conjectured analytic relationship.









\subsection{The nonstationary SAR model}

The nonstationary SAR model can be constructed by allowing the parameters $\kappa, H$, and $\sigma^2$ in the generating SPDE to vary over space. The SPDE becomes

$$ (\kappa^2(\mathbf s) - \nabla \cdot H(\mathbf s) \nabla)^{\nicefrac{\alpha}{2}} x(\mathbf s) = \mathcal{W}(\mathbf s) $$
where $\kappa(\mathbf s) > 0$, $\mathcal{W}(s) \sim \operatorname{WN}(0, \sigma^2(\mathbf s))$, and $\sigma^2(\mathbf s) > 0$. Discretizing this equation results in a valid GMRF, but it is unclear what covariance function the SAR approximates in this case.

In practice, one can use the discretization in (\ref{e:3}) to fill each row of the precision matrix individually, substituting the values for $\kappa$ and $H$ corresponding to that row's observation location. This gives a sparse banded precision matrix $Q$ with nonconstant bands. The covariance matrix implied by this precision matrix is nonstationary, and the sparse structure of the precision matrix can still be exploited as in the stationary case.

The process variance can also be allowed to vary in the same way as with the nonstationary Mat\'ern model, but this must be done balancing the effects of $\kappa$ and $H$. First $\kappa(\mathbf s)$ and $H(\mathbf s)$ must be encoded into the precision matrix. The implied marginal variances can then be computed for each location from the inverse of the precision matrix, and these variances can be adjusted to match the estimated marginal variance. 















\section{Local moving window likelihood estimation}


\subsection{Local estimation strategy}

Estimating a nonstationarity model can be challenging due to the increased number of covariance parameters. When enough data is available, however, local estimation can give insight into what type of nonstationarity is present.

Local estimation is usually accompanied by the assumption of approximate local stationarity. For this work, we define local stationarity and the local likelihood estimation technique for a Gaussian process with stationary Mat\'ern covariance as follows. First, divide the region of interest $\mathcal{D}$ into $M$ possibly overlapping subregions $\mathcal{D}_1, \mathcal{D}_2, \cdots, \mathcal{D}_M$. Then under the assumption of approximate local stationarity, we can model the data $\mathbf y_i$ within the subregion $\mathcal{D}_i$ using a Gaussian process $Y_i$ defined using the following specification:

\begin{equation}
\label{e:4}
Y_i(\mathbf s) = \mu_i(\mathbf s) + Z_i (\mathbf s) + \epsilon_i (\mathbf s)    
\end{equation}
where $\epsilon_i \sim \operatorname{WN}(0, \tau_i^2)$ is spatial white noise and $Z_i \sim \operatorname{GP}( \mathbf 0, F_i)$ is a spatially correlated Gaussian process with covariance function $F_i(\nu, a_i, \sigma_i^2)$ parameterized by a stationary Mat\'ern covariance function. Let $G_i = F_i + \tau_i^2$. The Gaussian process likelihood of $p$ replicates $\mathbf y_i$ which are flattened into a vector is

\begin{equation}
\label{e:5}
     \log L = - \frac{n p}{2} \log 2 \pi + \frac{1}{2} \log | G \ |^{-1} - \frac{1}{2}(\mathbf y_i - \boldsymbol \mu_i )^T G^{-1} (\mathbf y_i - \boldsymbol \mu_i)
\end{equation}
where $\boldsymbol{\mu}_i$ is the mean function $\mu_i$ evaluated at the locations of $\mathbf y_i$, and $\mathbf G$ is the covariance matrix for $\mathbf y_i$ which is block diagonal with blocks $G_i$. Without loss of generality assume $\boldsymbol \mu _i = \mathbf 0 \; \forall i$.

After partitioning the data, each local likelihood estimation is an embarrassingly parallel task, which makes it a viable strategy for large data sets with the help of many processors. If desired, one can assign the location of the center of the subregion $\mathcal{D}_i$ to the estimated parameters $a_i, \sigma_i^2, \tau_i^2,$, which effectively treats the MLEs as spatial fields that can be inspected and smoothed if necessary.


\subsection{Local estimation accuracy of the Mat\'ern range parameter}
\label{ss:2}

The natural questions when using local estimation techniques are how large of a window should be used and how many replicates of the spatial data are needed to give accurate results. To answer these questions, we designed a computer experiment testing whether covariance parameters can be accurately estimated when using a subset of data with independent replicates. We focus on the Mat\'ern covariance function in estimation because of its prevalence, flexibility, interpretability,  and the nice theoretical results that exist concerning estimation of the range and variance parameters \cite{kaufman2013role}. Furthermore, by defining the covariance matrix, the model isn't plagued by the edge effects of the SAR model. The goal is to answer how large of a correlation range can be estimated and to what accuracy, given a number of replicates generated with this range and given a certain window size. Using the generated replicates, we numerically find the MLE for the range and record the error of the estimate. 

The design of the computer experiment was chosen for practical considerations rather than a traditional design like Latin hypercube sampling. We chose to examine data generated from models with a Mat\'ern range that is one, two, three, and four times the size of the estimation window used. Window sizes were tested between a $5 \times 5$ grid and a $33 \times 33$ grid, and we used data sets with the number of replicates between 5 and 60. Thus, we have four grids (window size $\times$ number of replicates) based on how large the range is compared to the window size. For each combination of these three factors, we ran the optimization to find the MLE of the range parameter using the generated replicates. The optimization was repeated 100 times using different replicates to mitigate the effect of the random samples in the estimation error. This experiment was conducted twice with Mat\'ern smoothnesses $\nu =1, 2$. The quantity of interest is the percent error of the estimate. For each of the $4\times2$ grids, the 100 observations of percent error per location were used to fit thin plate spline models and predict the surfaces shown in Figure (\ref{f:2}) (using \texttt{fields::Tps} in \texttt{R}). 

\begin{figure}
    \centering
    \includegraphics[scale=0.25]{"plots/LocalTPS".png}
    \caption{Each panel displays the absolute percent error from estimating the Mat\'ern range parameter given a certain number of replicates and a window size (size of grid). Fixed Mat\'ern range parameters one, two, three, and four times the size of the grid were tested, faceted in panels (a)-(d). Thin plate splines were fit using the 100 repeated optimization results, performed at each grid location. The splines were used to predict the surfaces shown. The top row corresponds to $\nu=1$ and bottom to $\nu=2$. Note that white indicates $>50\%$ error}
    \label{f:2}
\end{figure}

The thin plate spline surfaces can be used as guidelines to decide how many replicates are necessary and what window size should be used to achieve a specific estimation error tolerance, given something is known about the size of the range to be estimated. These results are encouraging: e.g. only a small number of replicates ($>10$) are needed with a window size of $>10$ to estimate a range of 10. In the extreme case, a Mat\'ern range four times the size of the window might be estimated to within $10\%$ error if 30 replicates are available and using a window size of 10 or greater. Using these guidelines, we can be more confident that local moving window likelihood estimation is a viable technique if enough data is used.















\section{Data analysis}

In this section, we implement the methods studied in this paper to analyze a data set clearly exhibiting a nonstationary covariance structure. We first use moving window likelihood estimation to infer spatially varying Mat\'ern parameters. We then translate these into their local SAR covariance parameter equivalents, and we encode the spatially varying SAR parameters into the nonstationary SAR covariance model. This model makes it possible to visualize the resulting nonstationary covariance matrix and efficiently simulate new realizations.

\subsection{NCAR LENS Data}

The data set from the NCAR CESM Large Ensemble project \cite{kay2015community} is comprised of 30 spatial fields that we consider independent replicates of the same distribution due to the nature of climate model experiments run with different initial conditions. Nychka \cite{nychka2018modeling} first analyzed these data using the LatticeKrig model, and the original article details the climate science application. Details about the pattern scaling approach to statistical emulation can also be found in \cite{alexeeff2018emulating}. Briefly, each field is a measure of how the local temperature average is affected by a global temperature average increase of one degree Celsius. The data locations are on a $288 \times 192$ grid with approximately one degree resolution, covering the entire globe. 

\begin{figure}
    \centering
    \includegraphics[scale=0.275]{"plots/Estimates".png}
    \caption{The results of the moving window likelihood estimation. The sill $\sigma^2( \mathbf s)$ (a), nugget $\tau^2( \mathbf s)$ (b), geometric average range $\sqrt{\lambda_x ( \mathbf s) \lambda_y ( \mathbf s)}$ (c), and anisotropy ellipses $\Sigma ( \mathbf s)$ (d)}
    \label{f:3}
\end{figure}

To streamline this example, we focus on the subregion encompassing the Americas and surrounding oceans containing $13,052$ observations on a $102 \times 128$ grid. The top row of Figure \ref{f:4} shows the first four sample fields from the data set we analyze. The one data modification from \cite{nychka2018modeling} is that, in addition to de-meaning each grid box, we have also studentized the fields by dividing by the empirical standard deviation of each grid box over the 30 replicates. 

\subsection{Estimation}


First, we performed moving window likelihood estimation as specified in equations (\ref{e:4}) and (\ref{e:5}). We tested several window sizes between $8 \times 8$ and $15 \times 15$, and saw little change in the estimates.  Based on this range, an $11 \times 11$ window size was chosen. This window size is consistent with the long range correlations over the ocean and also the estimate from Section \ref{ss:2}. The estimation was performed on the NCAR Cheyenne supercomputer \cite{cheyenne} using the R programming language \cite{Rcore} with the \texttt{Rmpi} \cite{yu2002rmpi} and \texttt{fields} packages \cite{fields}. The details of the parallel implementation are the same as in \cite{nychka2018modeling}. Since the fields were studentized to begin with in this example, we included the constraint $\sigma^2 = 1 - \tau^2$.

The estimates for the spatially varying parameters are shown in Figure \ref{f:3}. The sill and nugget variances are shown in (a) and (b). Panel (c) shows the geometric mean of $\lambda_x$ and $\lambda_y$ as a measure of the ``average range", which agrees with the range in the isotropic case. Finally in panel (d), a sample grid of the estimated anisotropy matrices $\Sigma(\mathbf s_i)$ are plotted by computing the $15\%$ coverage of the bivariate Gaussian covariance ellipses using the \texttt{ellipse} package in R \cite{ellipse}.

The large signal to noise ratio $\sigma^2/\tau^2$ (not shown) and the evident transition in the covariance structure between land and ocean indicates that the nonstationarity in the second-order structure of the data is being accurately estimated. Based on the coastlines in some regions, we think that global estimation using some low-dimensional representations of the parameter fields could significantly influence the results; however, the decision is often application specific.



\subsection{Using local estimates}

\begin{figure}
    \centering
    \includegraphics[scale=0.26]{"plots/Simulations".png}
    \caption{The top row consists of the first four ensemble members from the NCAR CESM data set. The bottom row shows four unconditional simulations from the nonstationary SAR model}
    \label{f:4}
\end{figure}


The nonstationary SAR model is convenient for plugging in locally estimated parameters and capable of modeling large data sets. For this reason, we chose to translate the local Mat\'ern parameters into their approximate SAR parameter equivalents. The translation was done using the numerical relationship derived in this \ref{ss:1}. Then, the local SAR parameters were encoded into the nonstationary SAR model. Simulations from this covariance are shown in the bottom row of Figure \ref{f:4}. The simulations do a reasonable job emulating the data, but are lacking some of the long range anisotropy over the ocean.

To gain insight into how the nonstationary SAR model is related to the process convolution construction, we inspect the SAR model's correlations corresponding to individual observation locations, encoded in the rows of the correlation matrix, in the top row of Figure \ref{f:5}. Similarly, plotted in the bottom row of Figure \ref{f:5} are the rows of the symmetric square root of the correlation matrix, which give a discrete approximation to the kernels that could be used to ``construct" the process via the process convolution approach, analogous to $ \Psi_{\mathbf{s}}$. 


\begin{figure}
    \centering
    \includegraphics[scale=0.2]{"plots/Kernels".png} %width=300pt
    \caption{Correlations (top) and discrete approximate kernels (bottom) for four locations implied by the nonstationary Mat\'ern model. Note that the bottom row and far right column are at different resolutions}
    \label{f:5}
\end{figure}

Both anisotropy and nonstationarity are evident in Figure \ref{f:5}. Note the discontinuity off the eastern coast of South America in the discrete approximate kernel in the third column of the bottom row. This behavior is smoothed out and not seen in the corresponding correlation in the top row.


\begin{figure}
    \centering
    \includegraphics[scale=0.2]{"plots/Decorr".png} %width=300pt
    \caption{A decorrelated field corresponding to one of the spatial replicates in the data. The precision matrix $B$ applied to the data should result in white noise if the model captures the spatial distribution of the data.}
    \label{f:6}
\end{figure}

Finally, in Figure \ref{f:6} one can see the result of $B$, the symmetric square root of the precision matrix, applied to one of the replicates to which the model was fitted. To carry out this matrix multiplication, the spatial field is flattened into a vector in the same order specified by the covariance matrix. We used this diagnostic tool to visually assess the goodness of fit of the model covariance matrix to the spatial distribution of the data. If the spatial distribution of the data is fitted correctly, this process should result in a decorrelated field of white noise. Excluding the slight heteroskedasticity present near coastal regions, Figure \ref{f:6} indicates that the vast majority of the correlation in the data has been captured in the model, and therefore has been removed from the data via this matrix transformation. We did not implement any formal whitening test on the decorrelated fields, although this could be used as a more general goodness of fit test in covariance modeling.


\section{Conclusion}

In this paper, we have investigated the multistage framework of local estimation and global encoding. We have shown that when independent replicates of approximately locally stationary spatial data are available, robust local estimation is a viable technique for estimating the nonstationarity in the covariance parameters.

We also explored the stationary Mat\'ern-SAR covariance model approximation, conducting a numerical experiment to compare against existing results. It seems that the analytic approximation between the models is not exact for long correlation ranges, and we can use these numerical results to translate parameters between the Mat\'ern and SAR models more accurately.

An important contribution of this work is showing nonstationary data can be modeled by combining local maximum likelihood estimation with a simple global nonstationary covariance model that is straightforward to implement. We focused on encoding the locally estimated parameters in the nonstationary SAR model. In addition, the multistage approach is computationally efficient and can be applied to very large spatial data sets: local estimation avoids the big $n$ problem of global estimation, and encoding local estimates in a SAR model allows us to use sparsity for prediction and simulation. Another major advantage of this method is that it can be applied to both continuously indexed and lattice data. Local estimation of Mat\'ern parameters can incorporate regular or irregularly spaced data, and can always be performed in a way that makes encoding in the discrete SAR model possible.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%\section*{References}

\bibliography{Nonstat.bib}

\end{document}