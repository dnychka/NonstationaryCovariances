\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{verbatim}
\modulolinenumbers[5]
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{lineno}
\usepackage{units}
\linespread{1.1}
\input{NychkaStuff.tex}

\journal{Environmetrics}
\bibliographystyle{elsarticle-harv}

%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}

\title{Modeling nonstationary spatial data using local likelihood estimation and Mat\'ern-SAR covariance translation}
\tnotetext[t1]{This document is a collaborative effort. Declarations of interest for all authors: none}
 
\author[CU]{Ashton Wiens\corref{fn1}}
\ead{ashton.wiens@colorado.edu}
\cortext[fn1]{Corresponding author:
Department of Applied Mathematics, 
University of Colorado, 1111 Engineering Dr, Boulder, CO 80309}
\author[CSM]{Douglas Nychka}
\ead{nychka@mines.edu}
\author[CU]{William Kleiber}
\ead{william.kleiber@colorado.edu}
\address[CU]{Department of Applied Mathematics, University of Colorado, Boulder, Colorado, USA}
\address[CSM]{Department of Applied Mathematics and Statistics, Colorado School of Mines, Golden, Colorado, USA}

\address{Colorado, United States}

\begin{abstract}

Modeling data with a nonstationary covariance structure is important to represent heterogeneity in geophysical and other environmental spatial fields. However, spatial data sets with large numbers of locations are difficult to handle using standard methods.  A multistage approach to modeling nonstationary covariances is presented that is efficient for large data sets.  The key idea is to divide the modeling into two steps. First, we estimate spatial varying covariance parameters using a local likelihood applied to a moving spatial window. These parameters are then translated into an equivalent model as a spatial autoregression (SAR) that is global and sparse. The global feature of the SAR is a solution to the problem of how to combine the local estimates of the covariance into a single coherent description of a Gaussian process.  Moreover, This strategy combines the ease and interpretability of the Mat\'ern family of covariances and the efficient computation afforded by spatial models where the precision matrix is sparse. 
A main result is establishing the accuracy of approximating the Mat\'ern family of covariances with a SAR. We also demonstrate 
 through a Monte Carlo study that local likelihood estimates are accurate even for correlation ranges much larger than the window size provided a sufficient number of replicate fields are available.  This has important practical implications for keeping window sizes moderate. 
 The method is applied to an important suite of climate model simulations where replicated fields are available and the covariance varies significantly as the result of land/ocean effects.  Although this example benefits from spatial observations on a regular grid the methods are easily extended to irregular data using a basis expansion. 

\end{abstract}

\begin{keyword}
nonstationary Gaussian process \sep local likelihood \sep Gaussian Markov random field \sep spatial autoregression \sep process convolution
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}

This work is motivated by a climatological application where the goal is to emulate the variability of an ensemble of spatial fields generated by a climate forecast model. Accordingly, we are lead to model spatial data consisting of independent replicated spatial fields that exhibit a nonstationary covariance structure using a Gaussian process. To accurately emulate these fields, modeling the nonstationarity in the second-order structure of the data is essential. 
To avoid the computational limitations of representing and simulating nonstationary Gaussian processes, we investigate a two-step approach similar to the methodology in \cite{nychka2018modeling}. First, assuming the field is approximately locally stationary, we perform moving window local likelihood estimation to infer spatially varying Mat\'ern covariance parameters. These parameter fields are translated into the parameters of a spatial autoregressive model which best reproduces the behavior of the Mat\'ern correlations locally. Finally, the spatially varying parameters are encoded into the nonstationary SAR covariance model, specifying all of the data jointly.

Local estimation is not a new idea in spatial statistics \cite{haas1990kriging, haas1990lognormal, ver2004flexible, risser2015local}. A local estimation approach circumvents the $\mathcal{O}(n^3)$ computational burden, where $n$ is the number of observations. Instead, the task becomes $n$ embarrassingly parallel subproblems on the order of the window size used in the local estimation. 

We assume that local estimation is a data-driven approach: when the data consists of densely observed independent replicates, robust local estimation of covariance parameters is possible. In practice, there is often no clear indication of which parameters in the model should be allowed to vary spatially \cite{fuglstad2015does} and what spatial scales are appropriate for the parameter surfaces. This difficult modeling choice is avoided when using local estimation: we can allow all parameters to vary initially, and the local estimates will indicate whether the parameters are constant or vary over space. Furthermore, with local estimation, we do not have to decompose the parameter functions into some prespecified low-dimensional representation \cite{fuglstad2015exploring, risser2016nonstationary}, which can influence the estimation. 

Weighted local likelihoods have been studied to accommodate irregularly spaced observations \cite{anderes2011local}, but in this work we use a simple moving window applied to data on a lattice. Here, we focus on estimation of Mat\'ern parameters, primarily because of their interpretability and in order to study the relationship between the Mat\'ern and SAR covariance models, detailed below. To establish local estimation as a reliable technique, we use a Monte Carlo experiment to study the robustness of local estimation of the correlation range parameter. 

With locally estimated covariance parameters in hand, some care is required to combine these into a valid global nonstationary covariance model. A simple option is to use the estimates to construct local covariance functions and perform local simulation. However, a global covariance specifying the relationships among all of the data is desirable for efficient simulation and necessary for prediction. A global representation also avoids potential artifacts and ad hoc choices in synthesizing the spatial analysis across local windows.



There are several general classes of nonstationary models, such as deformation methods \cite{sampsonguttorp, anderes2008estimating}, basis function methods \cite{cressie2008fixed, katzfuss2011spatio, nychka2015multiresolution, nychka2002multiresolution}, process-convolution construction \cite{higdon1998process, higdon1999non, higdon2002space, paciorek2004nonstationary, fuentes2001new, fuentes2002spectral, zhu2010estimation}, and the SPDE approach \cite{lindgren2011explicit, lindgren2007explicit, simpson2012think, rue2005gaussian}. See \cite{risser2016nonstationary} for a review of nonstationary models and \cite{heaton2017methods} for a review of methods for large spatial data sets. Unfortunately, only a few of these methods are easily implemented due to the complexity of the models. Here, we investigate two existing nonstationary models from the process convolution and GMRF families of methods which are amenable to plug-in local estimates.

In this work, we study the nonstationary spatial autoregressive (SAR) model, related to the Gaussian Markov random field (GMRF) approach to approximating GPs. The idea is to identify members of the Mat\'ern family of spatial processes as solutions to a stochastic partial differential equation. The SPDE is then discretized to a lattice and this motivates the form of the SAR \cite{lindgren2011explicit}. The correspondence between the Mat\'ern/SPDE form and a SAR was presented in \cite{lindgren2011explicit}, and an analytical formula was proposed to connect the parameters between the continuous and discrete cases. We have found that the analytical formula is inaccurate for large correlation ranges and one contribution of this work is to sharpen this relationship using numerical results. The advantage is that if one can successfully translate the Mat\'ern formulation into a SAR framework, one can exploit sparse matrix algorithms for fast computation.


Finally, we apply this multistage modeling framework to analyze a nonstationary climate model output data set consisting of 30 temperature anomaly fields from the NCAR CESM project. First, we locally estimate stationary, anisotropic Mat\'ern parameters. We then translate these local Mat\'ern parameters into the SAR parameters which yield the best numerical approximation between the stationary Mat\'ern and the approximately stationary SAR model. Finally, we encode the spatially varying SAR parameters into the nonstationary SAR model. This model convincingly captures many of the nonstationary features of the climate distribution in simulations. 


The paper is organized as follows. In section 2, the Mat\'ern family of correlation functions and the process convolution model are introduced, along with the approximately stationary and nonstationary SAR models from the SPDE approach. We also include a numerical experiment investigating the link between the stationary Mat\'ern and SAR covariance models. In section 3, we develop the local likelihood framework we employ, and we conduct a local estimation simulation study.  In section 4, we apply the multistage modeling framework to analyze a nonstationary climatological data set. We conclude with a summary of the method and a discussion of some of the relevant practical aspects.














\section{Nonstationary covariance models}

 In this section, we introduce the Mat\'ern family of covariance models, as well as the process convolution and the SAR/SPDE approaches to constructing a Gaussian process/GMRF. We then state the connection between the Mat\'ern and SPDE models, and explore this relationship in a numerical study.


\subsection{The Mat\'ern covariance model}
\label{ss:2.1}

Let $f$ be a Gaussian process with mean zero and covariance function  $k( \bbx, \bbx^\prime)$.
The Mat\'ern family of stationary covariance models is important because of its flexibility and the interpretability of its parameters. The  Mat\'ern covariance function with a unit range (scale) parameter is

$$ \omega(d) = C( \, d \, | \, \nu,  \sigma^2 \, ) =\sigma ^{2}{\frac {2^{1-\nu }}{\Gamma (\nu )}}{( d)}^{\nu }\mathcal{K}_{\nu }{( d  )},$$



where $d$ is the Euclidean distance between  $\bbx$ and $\bbx^\prime$,  $\mathcal{K}_{\nu }(\cdot)$ is the modified Bessel function of the second kind of order $\nu$, and $\Gamma(\cdot)$ is the gamma function. $\sigma^2$ is the spatial process variance (sill), and $\nu$ is the smoothness parameter which controls the mean square differentiability of the process. The isotropic covariance function with range $\kappa$ is given by % $a$ is the multiplicative range parameter, 

$$ k( \bbx, \bbx^\prime) = \omega(\kappa d)$$

This model can be extended to include geometric anisotropy through the definition of the distance that is a linear scaling and rotation of the coordinates.
Let 
 $A  =  D^{-1} U^T$ be a  $2\times2$ matrix 
where $U$ is a rotation matrix parameterized by angle $\theta$

\[ U = \begin{bmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{bmatrix}, \]
and
$$D = \begin{bmatrix} \xi_x & 0 \\ 0 & \xi_y \end{bmatrix} $$ 
is a diagonal matrix  scaling the $x$ and $y$ coordinate axes separately . Then the pairwise Mahalanobis distances among the observation locations are defined as
$ d=  \| A\bbs - A\bbs^\prime \| $
and these distances are then used as the argument to the isotropic Mat\'ern covariance function. 
The interpretation is that if one transforms the coordinates according the linear transformation $A$ then the resulting field will be isotropic. 
% Also Wwth this notation we recover an isotropic covariance function setting  $\theta = 0$
%and 
%$  \xi_x = \xi_y$,  denoting this common scalar as  $\xi$.  

\subsection{Process convolution construction}

Process convolution is a useful method for constructing valid nonstationary GPs  using a  spatially varying kernel. 
  Let $\Psi( \mathbf u, \mathbf v)$  be a  kernel such that 
   \[ \int _ {G \times G} | \Psi (\mathbf u,\mathbf v) |^2 \mathrm d\mathbf u \mathrm d\mathbf v  < \infty , \]
   then we have the process representation 
$$ Y(\mathbf s) = \int_{G} \Psi(\mathbf s ,\mathbf u) \, \mathrm d W(\mathbf u ), $$ where $W$ is d-variate Brownian motion. It follows that $Y$ has covariance function 
\begin{equation}
\label{covarianceConvolution}
k(\mathbf s, \mathbf s^\prime) = \int _ {G}  \Psi (\mathbf s,\mathbf u)  \Psi (\mathbf s^\prime,\mathbf u)  d\mathbf u \
\end{equation}
 
The advantage of the convolution representation is that any choice of $\Psi$ will yield a valid covariance function, and this provides flexibility in devising dependence that varies over the spatial domain.  However, except in some special cases, the covariance  in (\ref{covarianceConvolution}) is difficult to compute in closed form from. To our knowledge only a Gaussian kernel is amenable to a closed form covariance function \cite{higdon1999non}. Given that parameter estimation and simulation require many evaluations of the covariance at pairs of locations the direct use of this model is problematic for large data sets. A key idea in this work is that shifting to a SAR process model gives an approximation to this convolution form but is also efficient to evaluate.  
This is due to a SAR giving rise to sparse precision matrices that approximate the inverses of the covariance matrices. 

We note that \citet{paciorek2004nonstationary} have also proposed a family of nonstationary covariance functions.  However, this construction deviates from the process convolution approach and  since it is in terms of covariances does not result in sparse matrices. It is an open question whether the SAR will approximate the form proposed in \cite{paciorek2004nonstationary}. 

\subsection{The SAR model}
\label{ss:SAR}

In contrast to modeling a continuous covariance function, the SAR model parameterizes the precision matrix for the process on a discrete lattice. In our case, we have observations of a spatial field $\mathbf{y}$ located on a regular rectangular grid. This is the setup used throughout the paper and in the data analysis section. 
We do not believe that this assumption is overly restrictive for development of this model and in the discussion we explain how to generalize this model to irregularly spaced observations and other linear functionals of the process. 

In two dimensions  assume the spatial process is indexed as $\bbY_{i,j}$  and without loss of generality take the integers  $1 \le i \le m$ and $1\le j \le n$  to be the lattice locations.    The 
isotropic SAR model can be written using graphical notation as 

\begin{equation}
\label{e:2}
    \begin{array}{c|c|c}
      0 & -1 & 0 \\
      \hline
      -1 & 4+ \kappa_S^2 & -1 \\
      \hline
      0 & -1 & 0 \\
    \end{array}
\end{equation} 
Given a white noise vector $\bbe_{i,j}$ distributed independent $\mathcal N(0, 1)$, we interpret the lattice process to satisfy 
 
\begin{equation}
\label{e:2.1}
 (4+\kappa_{S}^2) \bbY_{ij} - ( \bbY_{i-1,j} + \bbY_{i+1,j} + \bbY_{i,j-1} + \bbY_{i,j+1} ) =  \bbe_{i,j},
 \end{equation} 
with lattice values outside the range of the indices set to zero. 

Here $\kappa_{S}> 0 $ is suggestive of a range parameter controlling the dependence of the field and is similar but not identical to $\kappa$ for 
the continuous case. For this finite lattice one can identify a matrix $B$ using \ref{e:2.1} such that $B\bby =  \bbe$. Furthermore, it is clear that if $\kappa_{S}> 0$, the diagonal dominance of $B$ guarantees its invertibility. 
 With $\bby  =   B^{-1} \bbe$  the covariance matrix for $\bby$ is $ B^{-1}B^{-T}$ and thus $\bby$ has precision matrix $Q= B^T B $.  The precision matrix implied by the SAR model is sparse, but in two (or more) dimensions is not banded. The sparsity property makes the SAR model amenable to modeling large data sets because the precision matrix can be used instead of a dense covariance matrix for likelihood estimation and simulation.  Following the ideas from \cite{lindgren2011explicit} one can iterate the spatial autoregressive weights to obtain higher order models. For example, if $ B B \bby = \bbe $, this implies a SAR model extending to second-order nearest neighbors and gives the precision matrix: $Q_2= (B B)^T (B B)$.   The SAR model detailed here is a special case of a Gaussian Markov random field. For a given row of the precision matrix, the nonzero, off-diagonal entries index the neighbors that will determine the Markov property. Conditioning on these neighbors, the field at the lattice location (corresponding to the given row) will be independent of the remaining values of the field. For the first-order SAR described above the nonzero elements in $Q$ will include second-order neighbors. Thus this first-order SAR will be a GMRF based on second-order neighbors and the weights will depend on $B$. 

Two additional points  should be mentioned about the SAR model. First, this stencil should be modified at the boundaries of the domain. The center value of the stencil should be the $\kappa^2$ plus the sum of the weights of its non-zero neighbors. Second, the value of $\kappa$ affects the marginal variance of the process, so $\sigma^2$ is a parameter that allows for modulation of the variance, but it is not the marginal variance of the process itself. 

A version of the SAR model that exhibits approximate stationarity and also geometric anisotropy will be detailed in the next section. 
 


\subsection{Connection to Matern family}

%Lindgren, Rue, and Lindstr{\"o}m \cite{lindgren2011explicit} 

\citet{lindgren2011explicit} suggested that the SAR covariance model can be thought of as a discrete approximation to the Mat\'ern covariance model. The idea is that a Gaussian Markov random field with SAR covariance can provide an approximation to a Gaussian field with Mat\'ern covariance. The connection is established through an SPDE formulation. In particular, it is known that a Gaussian field $u(\mathbf s)$ with stationary Mat\'ern covariance is a solution to the SPDE

$$ (\kappa^2 - \Delta)^{\nicefrac{\alpha}{2}} u(\mathbf s) = \mathcal{W}(\mathbf s) $$
where $\alpha = \nu + \frac{d}{2}$, $\kappa > 0, \nu > 0$, $\mathbf s \in \Omega = \mathbb{R}^d$, $d = 1$ or $2$, and $\mathcal{W}(s)$ is identified as white noise with zero mean and variance $\sigma^2$. As in the Mat\'ern model, $\nu$ controls the smoothness of the Gaussian field. Fixing $\nu = 1$ and $d = 2$, the authors showed that the SAR covariance structure obtained by discretizing the pseudodifferential operator $(\kappa^2 - \Delta)$ approximates a Mat\'ern covariance structure with range $\kappa \approx \kappa_S$. This relationship is approximate.

Similar results can be obtained for different smoothness parameters $\nu$ by convolving the finite difference stencil in (\ref{e:2}) with itself $\nu$ times, as detailed in the previous section for $\nu=1$ and $\nu=2$.

 The SAR model can also be extended to incorporate geometric anisotropy. Let $H$  denote an anisotropy matrix and modify the  Laplacian in the pseudodifferential operator as follows

\begin{equation}
\label{SPDE}
 (\kappa^2 - \nabla \cdot H \nabla)^{\nicefrac{\alpha}{2}} u(\mathbf s) = \mathcal{W}(\mathbf s) 
 \end{equation} 
Here $H$ is assumed to be a symmetric positive-definite matrix.
 To avoid potential ambiguity we also identify  the Laplacian  operator above for two dimensions  in an expanded form as 
\[  \nabla \cdot H \nabla  \equiv  H_{1,1} \frac{\partial^2}{\partial^2 s_1} +  2 H_{2,1} \frac{\partial^2}{\partial s_1 \partial s_2} + H_{2,2} \frac{\partial^2}{\partial^2 s_2}. \]

From this, the first-order finite difference discretization of the anisotropic SPDE \ref{SPDE} gives the following stencil for filling the rows of the $B$ matrix

\begin{equation}
\label{e:3}
       \arraycolsep=6.0pt\def\arraystretch{2.5}
   \begin{array}{c|c|c}
      \frac{2H_{12}}{h_x h_y} & -\frac{H_{22}}{h_y^2} & -\frac{2H_{12}}{h_x h_y} \\
        \hline
      -\frac{H_{11}}{h_x^2} & \;\; \kappa^2 + \frac{2H_{11}}{h_x^2} + \frac{2H_{22}}{h_y^2} \;\; & -\frac{H_{11}}{h_x^2} \\
      \hline
      -\frac{2H_{12}}{h_x h_y} & -\frac{H_{22}}{h_y^2} & \frac{2H_{12}}{h_x h_y} \\
    \end{array} 
\end{equation}
where $h_x$ and $h_y$ are the grid spacings along the x-axis and y-axis. This is just a reparameterization of the results in Appendix A of \cite{lindgren2011explicit} which facilitates the practical translation of these models. Note that setting $h_x = h_y =1$, $H_{12} = H_{21} =0$, and $H_{11} = H_{22} = 1$ yields the first-order isotropic model from (\ref{e:2}).

%% Doug's original:
%Finally we connect  the role of $H$ in the SPDE formulation to the anisotropic model for the Matern.   Under the linear transformation $A=D^{-1}U^T$ from Section 2,  let $\bbs^* =  A^{-1}  \bbs $, let $u$ be an isotropic field  from the SPDE with Laplacian, $\nabla \cdot \nabla$,  and  set $u^*(\bbs^*) = u(A^{-1} \bbs^*)$. 
% Then from elementary properties of the gradient 
%\[ \nabla u^*( A \bbs*) =    \left. A^{-1} (\nabla u) \right| _{\bbs= A \bbs^*}  \]
% and so we have  
% \[ \nabla \cdot \nabla u  =  (A ^{-1}\nabla) \cdot A^{-1} \nabla u^*  =  (\nabla) \cdot A^{-T} A^{-1} \nabla u^* . \] 
% From this expression we identify $H= A^{-T}A^{-1}$.  From Section 2 if $u$ is an isotropic field then 
% $u^*$ will be anisotropic with coordinates transformed by $A^{-1}$. Moreover, $u^*$ will also be the solution to the SPDE with  $H= A^{-T}A^{-1}$. This connection provides guidance how to interpret $H$.   Note that if $A$ is a pure rotation then $H=I$ and isotropy is preserved.  
%%

Finally, we connect  the role of $H$ in the SPDE formulation to the anisotropic model for the Mat\'ern.   Under the linear transformation $A=D^{-1}U^T$ from Section \ref{ss:2.1},  let $\bbs^* =  A^{-1}  \bbs $, and let $u$ be an isotropic field solution to the SPDE with Laplacian $\nabla \cdot \nabla$. Furthermore, set $u^*(\bbs) = u(A^{-1} \bbs) = u(\bbs^*)$. 
 Then from elementary properties of the gradient 
\[ \nabla u^*( \bbs) = \nabla \left( u( A^{-1} \bbs) \right) =  \left. A^{-1} \nabla u(A^{-1} \bbs) \right| _{\bbs= A \bbs^*} =  \left. A^{-1} \nabla u( \bbs^*) \right| _{\bbs^*= A^{-1} \bbs}  \] %  \nabla u^*( A \bbs^*) = ?
 and so we have  
 \[ \nabla \cdot \nabla u^* =  (A ^{-1}\nabla) \cdot A^{-1} \nabla u(\bbs^*)  =  \left. \nabla \cdot A^{-T} A^{-1} \nabla u(\bbs^*) \right| _{\bbs^*= A^{-1} \bbs} . \] 
 From this expression we identify $H= A^{-T}A^{-1}$.  From Section \ref{ss:2.1}, if $u$ is an isotropic field then $u^*$ will be anisotropic with coordinates transformed by $A^{-1}$. Moreover, $u^*$ will also be the solution to the SPDE with $H= A^{-T}A^{-1}$. This connection provides guidance how to interpret $H$. Note that if $A$ is a pure rotation then $H=I$ and isotropy is preserved.  



\subsection{Numerical translation of range parameters between the Mat\'ern and SAR models}
\label{ss:1}

The connection between the anisotropic Matern family and a SAR relies on the approximation of a discretized Laplacian operator with finite differences of the fields on a lattice.  To provide an accurate statistical model, it is important to quantify this approximation and improve its calibration over the limiting expression suggested in \cite{lindgren2011explicit}. In this section we provide numerical evidence to show that an accurate calibration is possible if restricted to ranges of the covariance parameters. 

The  computational setup is as follows. Given a Mat\'ern range parameter $a$, we estimate the value of  $\kappa$ in the SAR model which gives the best approximation to the Mat\'ern correlation function.  We conducted this experiment with the smoothness of the Mat\'ern model fixed at $\nu=1$ and $\nu=2$, and with unit marginal variance for all models. The first step is to fix the Mat\'ern range parameter and evaluate a Mat\'ern correlation matrix on the grid. Then, we perform an optimization over $\kappa$ by encoding it into the SAR precision matrix using (\ref{e:2}), inverting and normalizing it to give the implied SAR correlation matrix, and minimizing the distance between these matrices by some measure.

It is known that the SAR covariance model suffers from edge effects. To avoid the interference of edge effects in this optimization , we quantify the difference between the two correlation matrices by only comparing the correlation of the lattice point  in the center of the grid for both models. 
For an $N \times N$ lattice of locations, with $N$ odd,  let  $\boldsymbol \sigma_a$ denote the  vector of correlations between the center point in this lattice  and all other locations based on the Mat\'ern covariance function.  Let  $\boldsymbol \sigma_\kappa$ be the same  correlation vector for the SAR model with parameter $\kappa$.  We then find \[ \displaystyle \min_{\kappa} \| \boldsymbol \sigma_a - \boldsymbol \sigma_\kappa \|_2 \]
Denote this minimzer $\hat{\kappa}( a)$ and so this value will be a mapping from the range parameter
of the Mat\'ern family into the SAR model. 


The approximation results are summarized in Figure \ref{f:1}. Here $a$ is varied over the interval $[1, 20]$, $N=51$ and the lattice points have unit spacing.  In \ref{f:1}(a)  $\hat{\kappa}( a)$  is plotting as a function of $a$. Orange corresponds to the $\nu=1$ case, cyan corresponds to $\nu=2$, and the solid black line shows the theoretical relationship, $\frac{1}{a} = \frac{1}{\kappa}$ from \cite{lindgren2011explicit}.  From these computations we conclude that at this level of discretization it is important not to rely on the analytic formula to translate between $a$ and $\kappa$ parameters. 


The relative error of using the SAR correlation with $\kappa$ value derived from the numerical experiment is shown in \ref{f:1}(d). The $\ell_2$ distance measure used in the optimization of the model correlation matrices is used to quantify the resulting model error, normalized by $\| \boldsymbol \sigma_a \|$. The relative error incurred when using $\frac{1}{\kappa} = \frac{1}{a}$ is shown by the dashed lines. Note that there is tenfold decrease in the relative error in many cases.




\begin{figure}
    \centering
    \makebox[0pt]{
        \includegraphics[scale=0.27]{"plots/MaternSARtranslation".png}
    }
    \caption{For the isotropic case, the optimal $1/ \kappa$ parameter for a given Mat\'ern inverse range $1/a$ is plotted in (a). The relative error incurred by using the SAR model with optimal $\kappa$ as an approximation to the Mat\'ern model is shown in (d). For the anisotropic case, the optimal eigenvalues of $H$ are plotted against the fixed diagonal values of $D^2$ in panels (b) and (c), and the relative error is shown in (e). Finally, the ratio of eigenvalues is shown in (f)}
    \label{f:1}
\end{figure}


%To make the Mat\'ern ranges comparable between the model with $\nu=1$ and $\nu=2$, we used the decorrelation range as a proxy. Specifically, for each Mat\'ern range and fixing $\nu=1$, we found the distance at which correlation dropped to 0.05. Then, we found the range of Mat\'ern with smoothness $\nu=2$ which also decorrelated to 0.05 at the same distance. Note that for comparison we have plotted the smoothness $\nu=2$ against the $\nu=1$ range parameters since they are equivalent in the sense just described.

In the data analysis below, we found it necessary to include geometric anisotropy in the covariance model. For this reason, we also investigated how the presence of geometric anisotropy affects the numerical correspondence established for the isotropic case above. Overall, the behavior of the approximation was similar to the isotropic case: the estimated eigenvalues of the SAR anisotropy matrix $H$ were smaller than the eigenvalues fixed in the Mat\'ern anisotropy $D^2$ matrix. In this experiment, we encoded fixed values for $\xi_x$ and $\xi_y$ in a Mat\'ern correlation matrix such that the length scale ratio $\xi_x \colon \hspace{-0.4mm}  \xi_y = 4 \colon \hspace{-1mm} 1$, which was consistent with the anisotropic estimates in the data analysis. In particular, we let $\xi_x = 1, \cdots, 20$ and $\xi_y = 4\xi_x$. Then, the optimal eigenvalues of the SAR anisotropy matrix $H$ were found. The experiment was repeated with rotation angles $\theta = 0^{\circ}, 10^{\circ}, \cdots, 90^{\circ}$, and the rotation angle was assumed to be known and fixed in the eigendecomposition of the $H$ matrix. The anisotropic parameter translation results for $\xi_x$ and $\xi_y$ are shown in panels (b) and (c) of Fig \ref{f:1}, respectively, and the relative error of approximation is shown in panel (e). Finally, the ratio of the estimated eigenvalues of $H$ is shown in panel (f); compare to the line $y=4$.

The approximation results may be slightly affected by the rotation angle and oblateness of the geometric anisotropy, but the effect is negligible in practice. From these results, we have ascertained a numerical translation among the anisotropy parameters. We can use these results to translate locally estimated Mat\'ern range parameters into SAR parameters with better accuracy than the conjectured analytic relationship.









\subsection{The nonstationary SAR model}

The nonstationary SAR model can be constructed by allowing the parameters $\kappa, H$, and $\sigma^2$ in the generating SPDE to vary over space. 
Let
 \[
  \cL(\bbs) = H_{1,1}(\bbs) \frac{\partial^2}{\partial s_1^2} +  2 H_{2,1}(\bbs)\frac{\partial^2}{\partial s_1 \partial s_2} + H_{2,2}(\bbs) \frac{\partial^2}{\partial s_2^2}.
\]
The SPDE can then be written as
$$ (\kappa^2(\mathbf s) - \cL(\bbs) )^{\nicefrac{\alpha}{2}} u(\mathbf s) = \mathcal{W}(\mathbf s) $$
where $\kappa(\mathbf s) > 0$, $\mathcal{W}(s) \sim \operatorname{WN}(0, \sigma^2(\mathbf s))$, and $\sigma^2(\mathbf s) > 0$.  Furthermore, we specialize to a spatially varying linear transformation of the coordinates, $A(\bbs)$, and so  $H(\bbs) = A^{-T} (\bbs) A^{-1}(\bbs)$. Note that $A(\bbs)$ varying in space is equivalent to specifying spatial fields for $\theta$, $\xi_x$ and $\xi_y$ within $U$ and $D$. 

Discretizing this equation results in a valid GMRF that is nonstationary. In particular, the autoregressive $B$ matrix from Section \ref{ss:SAR} could have  different elements in each row based on the variation in $H(\bbs)$ or $\kappa(\bbs)$. However, $B$ will still be a sparse matrix and $Q$ will be positive-definite. One can connect the matrix $B^{-1} $ as a discretization of a kernel $\Psi$ to the lattice points, as well as the covariance matrix $B^{-1}B^{-T}$ as a discrete approximation to the integral in (\ref{covarianceConvolution}). 

The process variance can also be allowed to vary in the same way as with the nonstationary Mat\'ern model, but this must be done balancing the identifiability of $\kappa$ and $H$ and the fact that edge effects may introduce spurious variation in the  GMRF variance. Our approach is to first construct the precision matrix and then use sparse matrix methods to solve for the diagonal elements of the covariance matrix. The rows of $B$ are then weighted so that  this new version gives a GMRF with constant marginal variance. With this normalization of the SAR model $\sigma(\bbs)^2$ can be introduced to capture explicit spatial variation in the process marginal variance. 

\section{Local moving window likelihood estimation}
\subsection{Local estimation strategy}

Estimating a nonstationarity model can be challenging due to the increased number of covariance parameters. When enough data is available, however, local estimation can give insight into what type of nonstationarity is present. Moreover, we show in this section that a modest  number of spatial replicate fields results in stable local covariance estimates. 

Local estimation is usually accompanied by the assumption of approximate local stationarity. For this work, we define local stationarity and the local likelihood estimation technique for a Gaussian process with stationary Mat\'ern covariance as follows. First, divide the region of interest $\mathcal{D}$ into $M$ possibly overlapping subregions, or {\it windows},  $\mathcal{D}_1, \mathcal{D}_2, \cdots, \mathcal{D}_M$. Then under the assumption of approximate local stationarity, we can model the data $\mathbf y_i$ within the subregion $\mathcal{D}_i$ using a Gaussian process $Y_i$ defined using the following specification:

\begin{equation}
\label{e:4}
\bby_i(\mathbf s) = \mu_i(\mathbf s) + Z_i (\mathbf s) + \epsilon_i (\mathbf s)    
\end{equation}
where $\epsilon_i \sim \operatorname{WN}(0, \tau_i^2)$ is spatial white noise, and $Z_i \sim \operatorname{GP}( \mathbf 0, F_i)$ is a spatially correlated Gaussian process with covariance matrix $F_i$ parameterized by an anisotropic but  stationary Mat\'ern covariance function. Let $G_i = F_i + \tau_i^2 I$. The approximate Gaussian process likelihood $L$  based on  $p$ replicates $\mathbf y_i$ is
\begin{equation}
\label{e:5}
     \log L(\nu, \bxi_i, \theta_i,  \sigma_i)  = - \frac{n p}{2} \log 2 \pi + \frac{1}{2} \log | G \ |^{-1} - \sum_{i=1}^p \frac{1}{2}(\mathbf y_i - \boldsymbol \mu_i )^T G_i^{-1} (\mathbf y_i - \boldsymbol \mu_i)
\end{equation}
where $\boldsymbol{\mu}_i$ is the mean function $\mu_i$ evaluated at the locations of $\mathbf y_i$, and $G_i$ is the covariance matrix for $\mathbf y_i$ and depends implicitly on the parameters enumerated in the likelihood. The likelihood is approximate because we are assuming stationarity within each data window.  % $\mathbf G$ ?

After partitioning the data, finding each local likelihood estimate is an embarrassingly parallel task,
 which makes it a viable strategy for large data sets by using many computational cores. In fact, in our application the parallelization is efficient to the point that we take  the subregions to be an exhaustive set of moving windows centered at every grid point. 
We assign these estimated parameters to the location of the center of the subregion $\mathcal{D}_i$, and
after translating into the SAR parameterization these specify the row of $B$, the SAR matrix, at this location. This assignment is, of course, predicated on the assumption that over the region there is little variation in these parameters. This issue will be discussed in more detail in the last section. 

Given that the SAR model also gives a specification of the covariance it may seem indirect that the local estimates focus on the Mat\'ern model, and then the estimates are transformed into the SAR representation. An alternative would be to estimate the SAR version directly from local likelihood windowing. There are several  reasons for the two-step procedure. Fitting the covariance model directly avoids any boundary effects that would come about by applying the SAR to a small window. Also, the local fitting is by definition small in size and sparse matrix methods associated with the SAR will not be as efficient as the direct ML estimation using dense covariance matrices.  Finally, the Mat\'ern parameters are easier to interpret and will be more simple to model in a hierarchical statistical framework. 

\subsection{Estimation accuracy of the Mat\'ern range parameter}
\label{ss:2}
A practical issue for a local approach, especially in the context of determining covariance parameters, is whether the number of replicates and the size of the window are adequate for robust estimation of  parameters. 
Although choosing a data adaptive window is beyond the scope of this work, it is important to identify the conditions under which parameter estimates will be accurate. As a side issue, it is also useful to understand the benefits of replicate spatial fields in estimating a covariance model. In particular, the hope is that replication makes it possible to estimate correlation ranges that are much larger than the local window size. 
Here we  focus on the Mat\'ern covariance family in estimation because of its prevalence, flexibility, interpretability, and the useful theoretical guidance  concerning estimation of the range and variance parameters \cite{kaufman2013role}. 

The Monte Carlo experiment is organized with four factors: window size ranging between  $5 \times 5$ grid and a $33 \times 33$ grid, a Mat\'ern range parameter being multiples of $1,2,3,$ and $4$ times the window size, the  Mat\'ern smoothness parameter taking on values $1$ and $2$, and the number of replicates ranging between 5 and 60.  Thus the full factorial design is $11 \times 4 \times 2 \times 9$ (window size $\times$ range parameter $\times$ smoothness parameter $\times$ replicates). 
For each combination, replicates with given range and smoothness were simulated with given window size, and the Mat\'ern range was estimated using maximum likelihood. This was done 100 times for each combination, and statistics were assembled from the 100 independent MLEs for the range parameter. The  main quantity of interest is the percent error of the estimate, and so percent error surfaces as a function of replicate number and window size are summarized in Figure \ref{f:2}.  In this experiment, the range parameter was varied based on the window size which may seem unusual.  However, the motivation was to address the computational requirements of the problem: given a computational budget to accommodate windows of a specific size, what size range parameter can be accurately estimated? Note that with constraints on the window size, accuracy can also be improved by increasing the number of replicates. 

\begin{figure}[h]
    \centering
   \includegraphics[scale=0.23]{"plots/LocalTPS".png}
    \caption{Each panel displays the absolute percent error from estimating the Mat\'ern range parameter given a certain number of replicates and a window size (size of grid). Fixed Mat\'ern range parameters one, two, three, and four times the size of the grid were tested, faceted in columns (a)-(d). The top row corresponds to $\nu=1$ and bottom to $\nu=2$. Thin plate splines were fit using the 100 repeated optimization results, performed at each grid location. The splines were used to predict the surfaces shown. Note that white indicates $>50\%$ error}
    \label{f:2}
\end{figure}

These surfaces can be used as guidelines to decide how many replicates are necessary and what window size should be used to achieve a specific estimation error tolerance, given something is known about the size of the range to be estimated. The results are encouraging; e.g. only a small number of replicates ($>10$) are needed with a window size of $>10$ to estimate a range of 10. In the extreme case, a Mat\'ern range four times the size of the window might be estimated to within $10\%$ error if 30 replicates are available and using a window size of 10 or greater. Using these guidelines, we can be more confident that local moving window likelihood estimation is a viable technique if enough data is used.





\begin{comment}
\section{Data analysis}

In this section, we implement the methods studied in this paper to analyze a data set where it is important to account for nonstationary covariance structure. We first use moving window likelihood estimation to infer spatially varying Mat\'ern parameters. We then translate these into their local SAR covariance parameter equivalents, and we encode the spatially varying SAR parameters into the nonstationary SAR covariance model. This model makes it possible to visualize the resulting nonstationary covariance matrix and efficiently simulate new realizations.
\end{comment}

\section{NCAR LENS Data}

The data set from the NCAR CESM Large Ensemble project \cite{kay2015community} is comprised of 30 spatial fields that can be assumed to be independent replicates from the same distribution. This feature is based on the nature of  climate model experiments run over a long period and started with different initial conditions. \citet{nychka2018modeling} first analyzed these data using the LatticeKrig model, and the original article details the climate science application. The data locations are on a $288 \times 192$ grid with approximately one degree resolution, covering the entire globe.  Details about the pattern scaling approach to statistical emulation can also be found in \cite{alexeeff2018emulating}. Briefly, each field is a measure of how the local temperature average is affected by a global temperature average increase of one degree Celsius.  Generating this ensemble involved a large amount of supercomputer resources. 
The statistical task is represent these spatial fields with a probability distribution where is more efficient to generate additional fields (e.g. several hundred or thousands) that track the original 30 member model results. 

\begin{figure}
    \centering
    \includegraphics[scale=0.275]{"plots/Estimates".png}
    \caption{The results of the moving window likelihood estimation. The sill $\sigma^2( \mathbf s)$ (a), nugget $\tau^2( \mathbf s)$ (b), geometric average range $\sqrt{\xi_x ( \mathbf s) \xi_y ( \mathbf s)}$ (c), and anisotropy ellipses defined by $A^T ( \mathbf s)A ( \mathbf s)$ (d)}
    \label{f:3}
\end{figure}

To streamline the example, we focus on the subregion including the Americas and surrounding oceans containing $13,052$ observations on a $102 \times 128$ grid. The top row of Figure \ref{f:4} shows the first four sample fields from the data set we analyze. The one data modification from \cite{nychka2018modeling} is that, in addition to subtracting the ensemble mean from each grid box, we have also standardized the fields by dividing by the ensemble standard deviation of each grid box.

\subsection{Covariance parameter estimates}

 Moving window local MLEs were found using  window sizes between $8 \times 8$ and $15 \times 15$. Among these choices there was little change in the estimates and subsequent analysis used a $9 \times 9$ window. This window size is consistent with the long range correlations over the ocean and also the estimate from Section \ref{ss:2}. The estimation was performed on the NCAR Cheyenne supercomputer \cite{cheyenne} using the R programming language \cite{Rcore} with the \texttt{Rmpi} \cite{yu2002rmpi} and \texttt{fields} packages \cite{fields}. The details of the parallel implementation are the same as in \cite{nychka2018modeling}. Since the fields were standardized, the constraint $\sigma^2 = 1 - \tau^2$ was included. 

The estimates for the spatially varying parameters are shown in Figure \ref{f:3}. The sill and nugget variances are shown in (a) and (b). Panel (c) shows the geometric mean of $\xi_x$ and $\xi_y$ as a measure of the average correlation range, and this also agrees with the range in the isotropic case. Finally in panel (d), the estimated anisotropy matrix $A(\mathbf s_i)$ is depicted by glyphs indicating the range and departure from isotropy. 
The large signal to noise ratio $\sigma^2/\tau^2$ (not shown) and the evident transition in the covariance structure between land and ocean suggests that the nonstationarity in the second-order structure of the data is being accurately estimated. Based on the coastlines in some regions, we think that the addition of a land/ocean covariate may be useful.



\subsection{Model checking}

\begin{figure}
    \centering 
   \includegraphics[scale=0.28]{"plots/Simulations".png}
    \caption{The top row consists of the first four ensemble members from the NCAR CESM data set. The bottom row shows four unconditional simulations from the nonstationary SAR model}
    \label{f:4}
\end{figure}



The nonstationary SAR model is convenient for plugging in locally estimated parameters and capable of modeling large data sets. For this reason, we chose to translate the local Mat\'ern parameters into their approximate SAR parameter equivalents. The translation was done using the numerical relationship derived in this \ref{ss:1}. Then, the local SAR parameters were encoded into the nonstationary SAR model. Simulations from this covariance are shown in the bottom row of Figure \ref{f:4}. The simulations do a reasonable job emulating the data, but are lacking some of the long range anisotropy over the ocean.


%To illustrate how the nonstationarity estimated for this model is related to land/ocean boundaries, Figure \ref{f:5} shows several different locations in the spatial domain and plots the SAR weights (different rows of $B$). Similarly, plotted in the bottom row of Figure \ref{f:5} are the rows of the symmetric square root of the correlation matrix, which give a discrete approximation to the kernels that could be used to ``construct" the process via the process convolution approach, analogous to $ \Psi_{\mathbf{s}}$. 

To illustrate how the nonstationarity estimated for this model is related to land/ocean boundaries, Figure \ref{f:5} shows several different locations in the spatial domain and plots the correlations implied by the nonstationary SAR model. Both anisotropy and nonstationarity are evident in Figure \ref{f:5}. %Note the discontinuity off the eastern coast of South America in the discrete approximate kernel in the third column of the bottom row. This behavior is smoothed out and not seen in the corresponding correlation in the top row.



\begin{figure}
    \centering
    \includegraphics[scale=0.28]{"plots/Correlations".png} %width=300pt % 0.2
    \caption{Correlations corresponding to three observation locations at the same latitude implied by the nonstationary SAR model}
    \label{f:5}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[scale=0.3]{"plots/Decorr".png} %width=300pt
    \caption{Three decorrelated fields corresponding to successive applications of matrices to one of the spatial replicates in the data. In (a), the symmetric square root of the precision matrix $B$ has been applied to a data replicate $\mathbf y$ yielding $\mathbf w = B \mathbf y$. The first-order approximation is then applied in (b) giving $\mathbf v_1 =  (I - \frac{1}{2} \tau^2 Q)\mathbf w$, and the second-order approximation $\mathbf v_2 =  (I - \frac{1}{2} \tau^2 Q - \frac{3}{8} \tau^4 Q^2)\mathbf w$ is shown in (c).  }
    \label{f:6}
\end{figure}


The SAR representation as a global model for  the spatial field provides a convenient way to check the model fit.   Under the assumption that the nugget variance is small relative to the smooth Gaussian process, the SAR matrix should  decorrelate the observed field. 
Let $\bby$ be the observed field with covariance matrix $\Sigma$. The simple idea is to factor $\Sigma$ as $A^{-1} A^{-T}$ and then  check that $A\bby$ is a white noise field, or at least a spatial process with greatly reduced  spatial dependence. 
Note that the choice of $A$ is not unique, but it makes sense to choose a version of the square root that has weights that are localized around each observation location. In this analysis 
  \[ \Sigma=  \sigma^2 B^{-1} B^{-T} + \tau^2 I  = \sigma B^{-1}( I + \tau^2 Q ) \sigma B^{-T}, \]
where $Q$ is the precision matrix. If the $\tau= 0$ then the SAR matrix provides a transformation to white noise that is justified, and its approximate will be due just to the local stationary assumption in the fitting and the translation from the Matern to the SAR, which are both negligible as we have shown.
then the SAR matrix gives a version of $A$ that is localized. MORE EXPLANATION
If $\tau^2$ is small relative to $\sigma^2$, then $B\bby$ will have covariance  $(I +  \tau^2Q)$ and will approximate a white noise field. Small $\tau^2$ is a reasonable assumption in practice because one is often interested in simulation and prediction of spatial data that has strong spatial coherence. Note that $Q$ is sparse and when viewed as a covariance matrix will have localized, finitely supported correlations. Avoiding explicitly calculating $A$ (which requires dense matrix computation), we propose a refinement of just using $B$ as a decorrelation transformation: in addition, approximate the inverse square root of $( I + \tau^2 Q )$ with a series expansion to find a more accurate approximation to $A$. 
From elementary power series expansions we have 
\[ 1/\sqrt{ 1 + u} =  1  - \frac{1}{2}u + \frac{1\cdot 3}{2 \cdot 4} u^2 - \frac{1 \cdot 3 \cdot 5}{2 \cdot 4 \cdot 6} u^3 + \ldots, \]
and provided that  $\| \tau^2 Q \| < 1$  one can use this power series to approximate  $(I + \tau^2 Q)^{-1/2}$ as a symmetric matrix. For example, a first-order correction would be $I - (1/2) \tau^2 Q $ and it easily applied since $Q$ is formed from the sparse SAR matrix. Thus, for transforming a limited number of fields one would never construct the matrix explicitly, but rather just multiply the fields of interest by successive power of $Q$.

Figure \ref{f:6}(b) depicts the result of $B$ applied to one of the replicates (shown in (a)) to which the model was fitted. To carry out this matrix multiplication, the spatial field is flattened into a vector in the same order specified by the covariance matrix.  In panels (c) and (d), the first- and second-order corrections were applied to the field in (b). As a diagnostic tool one can visually assess the goodness-of-fit of the model covariance matrix to the spatial distribution of the data using these techniques. If the spatial distribution of the data is fitted accurately, this process should result in a decorrelated field of white noise. Excluding the slight heteroskedasticity present near coastal regions, Figure \ref{f:6} indicates that the vast majority of the correlation in the data has been captured in the model, and therefore has been removed from the data via these matrix transformations. This success is encouraging given the long range correlations over the ocean that have been identified from local estimation, and as a SAR only operates on second-order neighbors.  A formal test of independence was not implemented on the decorrelated fields, although this could be used as a more general goodness-of-fit test in covariance modeling.


\section{Conclusion}

In this paper, we have investigated a two-step framework of local estimation and global encoding to represent large and nonstationary covariance functions. We have shown that when independent replicates of spatial data are available where locally stationarity holds, local ML estimation is a robust technique for estimating the nonstationarity in the covariance parameters. in particular, the Monte Carlo results indicate the climate model example falls within this context. 

We also explored the stationary Mat\'ern-SAR covariance model approximation, conducting a numerical experiment to compare against existing results. The analytic approximation between the models is not reliable for long correlation ranges; however, we can use a numerical approximation to translate parameters between the Mat\'ern and SAR models more accurately. To our knowledge this is the first time detailed numerical mappings have been made between the anisotropic SAR model and an anisotropic Mat\'ern covariance function. 

An important contribution of this work is showing nonstationary data can be modeled by combining local maximum likelihood estimation with a simple global nonstationary covariance model that is straightforward to implement. We focused on encoding the locally estimated parameters in the nonstationary SAR model. In addition, the multistage approach is computationally efficient and can be applied to very large spatial data sets: local estimation avoids the big $n$ problem of global estimation, and encoding local estimates in a SAR model allows us to exploit sparsity for prediction and simulation. Another major advantage of this method is that it can be applied to both continuously indexed and lattice data. 
To reduce the scope of this work we have focused on lattice data. Although this restricted format will continue to be standard for climate models, the SAR model can also be extended to irregularly spaced data. One approach for non-lattice spatial data is the LatticeKrig model that imposes the SAR and lattice structure on coefficients in a basis function expansion rather than directly on the field.  We believe the anisotropic models developed here will carry over for more general models such as basis expansions, and the inverse square root transformation will be an important diagnostic tool for nonstationary modeling. 

DISCUSS: how to generalize this model to irregularly spaced observations and other linear functionals of the process. 
 
 AND ADD THIS TO INTRODUCTION
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%\section*{References}

\bibliography{Nonstat.bib}

\end{document}